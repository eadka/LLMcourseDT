{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a75230de-8ba9-4748-a3fa-b326ea4207df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: dlt\n",
      "Version: 1.12.3\n",
      "Summary: dlt is an open-source python-first scalable data loading library that does not require any backend to run.\n",
      "Home-page: https://github.com/dlt-hub\n",
      "Author: \n",
      "Author-email: \"dltHub Inc.\" <services@dlthub.com>\n",
      "License-Expression: Apache-2.0\n",
      "Location: /usr/local/python/3.12.1/lib/python3.12/site-packages\n",
      "Requires: click, fsspec, gitpython, giturlparse, hexbytes, humanize, jsonpath-ng, orjson, packaging, pathvalidate, pendulum, pluggy, pytz, pyyaml, requests, requirements-parser, rich-argparse, semver, setuptools, simplejson, sqlglot, tenacity, tomlkit, typing-extensions, tzdata\n",
      "Required-by: cognee\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show dlt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03ec0481-f2e4-49a5-933c-5d1812383172",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'colab' from 'google' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m colab\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      4\u001b[39m os.environ[\u001b[33m\"\u001b[39m\u001b[33mLLM_API_KEY\u001b[39m\u001b[33m\"\u001b[39m] = userdata.get(\u001b[33m'\u001b[39m\u001b[33mLLM_API_KEY\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;66;03m# it can be OpenAI API key\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'colab' from 'google' (unknown location)"
     ]
    }
   ],
   "source": [
    "from google import colab\n",
    "import os\n",
    "\n",
    "os.environ[\"LLM_API_KEY\"] = userdata.get('LLM_API_KEY') # it can be OpenAI API key\n",
    "os.environ[\"GRAPH_DATABASE_PROVIDER\"] = \"kuzu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11b68a2f-aae2-4ed2-b487-240ca13051fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "PipelineStepFailed",
     "evalue": "Pipeline execution failed at `step=sync` with exception:\n\n<class 'dlt.common.exceptions.MissingDependencyException'>\n\nYou must install additional dependencies to run `duckdb destination`. If you use pip you may do the following:\n\npip install \"dlt[duckdb]\"\n\nDependencies for specific destinations are available as extras of dlt",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/destinations/dataset/utils.py:92\u001b[39m, in \u001b[36mget_destination_clients\u001b[39m\u001b[34m(schema, destination, destination_dataset_name, multi_dataset_default_schema_name, staging, staging_dataset_name)\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# create instance with initial_config properly set\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m client = \u001b[43mdestination\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m client, staging_client\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/common/destination/reference.py:181\u001b[39m, in \u001b[36mDestination.client\u001b[39m\u001b[34m(self, schema, initial_config)\u001b[39m\n\u001b[32m    177\u001b[39m     schema.naming.max_length = \u001b[38;5;28mmin\u001b[39m(\n\u001b[32m    178\u001b[39m         caps.max_identifier_length \u001b[38;5;129;01mor\u001b[39;00m caps.max_column_identifier_length,\n\u001b[32m    179\u001b[39m         caps.max_column_identifier_length \u001b[38;5;129;01mor\u001b[39;00m caps.max_identifier_length,\n\u001b[32m    180\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient_class\u001b[49m(schema, config, caps)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/destinations/impl/duckdb/factory.py:161\u001b[39m, in \u001b[36mduckdb.client_class\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclient_class\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Type[\u001b[33m\"\u001b[39m\u001b[33mDuckDbClient\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdlt\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdestinations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimpl\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mduckdb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mduck\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DuckDbClient\n\u001b[32m    163\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m DuckDbClient\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/destinations/impl/duckdb/duck.py:19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdlt\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdestinations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minsert_job_client\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InsertValuesJobClient\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdlt\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdestinations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimpl\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mduckdb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql_client\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DuckDbSqlClient\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdlt\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdestinations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimpl\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mduckdb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfiguration\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DuckDbClientConfiguration\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/destinations/impl/duckdb/sql_client.py:3\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mduckdb\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msemver\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'duckdb'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mMissingDependencyException\u001b[39m                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/pipeline/pipeline.py:780\u001b[39m, in \u001b[36mPipeline._sync_destination\u001b[39m\u001b[34m(self, destination, staging, dataset_name)\u001b[39m\n\u001b[32m    778\u001b[39m restored_schemas: Sequence[Schema] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m780\u001b[39m remote_state = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_restore_state_from_destination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    782\u001b[39m \u001b[38;5;66;03m# if remote state is newer or same\u001b[39;00m\n\u001b[32m    783\u001b[39m \u001b[38;5;66;03m# print(f'REMOTE STATE: {(remote_state or {}).get(\"_state_version\")} >= {state[\"_state_version\"]}')\u001b[39;00m\n\u001b[32m    784\u001b[39m \u001b[38;5;66;03m# TODO: check if remote_state[\"_state_version\"] is not in 10 recent version. then we know remote is newer.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/pipeline/pipeline.py:1572\u001b[39m, in \u001b[36mPipeline._restore_state_from_destination\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1571\u001b[39m     schema = Schema(schema_name)\n\u001b[32m-> \u001b[39m\u001b[32m1572\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_destination_clients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mas\u001b[39;00m job_client:\n\u001b[32m   1573\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(job_client, WithStateSync):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/pipeline/pipeline.py:1320\u001b[39m, in \u001b[36mPipeline._get_destination_clients\u001b[39m\u001b[34m(self, schema)\u001b[39m\n\u001b[32m   1312\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PipelineConfigMissing(\n\u001b[32m   1313\u001b[39m         \u001b[38;5;28mself\u001b[39m.pipeline_name,\n\u001b[32m   1314\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdestination\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1317\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m directly or via .dlt/config.toml file or environment variable.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1318\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1320\u001b[39m destination_client, staging_client = \u001b[43mget_destination_clients\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1321\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdefault_schema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1322\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdestination\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_destination\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdestination_dataset_name\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1324\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstaging\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_staging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# in case of destination that does not need dataset name, we still must\u001b[39;49;00m\n\u001b[32m   1326\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# provide one to staging\u001b[39;49;00m\n\u001b[32m   1327\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# TODO: allow for separate staging_dataset_name, that will require to migrate pipeline state\u001b[39;49;00m\n\u001b[32m   1328\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#   to store it.\u001b[39;49;00m\n\u001b[32m   1329\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstaging_dataset_name\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_dataset_name\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_staging\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1330\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmulti_dataset_default_schema_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdefault_schema_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43muse_single_dataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m   1332\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1335\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(destination_client.config, DestinationClientStagingConfiguration):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/destinations/dataset/utils.py:100\u001b[39m, in \u001b[36mget_destination_clients\u001b[39m\u001b[34m(schema, destination, destination_dataset_name, multi_dataset_default_schema_name, staging, staging_dataset_name)\u001b[39m\n\u001b[32m     99\u001b[39m client_spec = destination.spec()\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m MissingDependencyException(\n\u001b[32m    101\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclient_spec.destination_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m destination\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    102\u001b[39m     [\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mversion.DLT_PKG_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclient_spec.destination_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    103\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mDependencies for specific destinations are available as extras of dlt\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    104\u001b[39m )\n",
      "\u001b[31mMissingDependencyException\u001b[39m: \nYou must install additional dependencies to run `duckdb destination`. If you use pip you may do the following:\n\npip install \"dlt[duckdb]\"\n\nDependencies for specific destinations are available as extras of dlt",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mPipelineStepFailed\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Step 2: Create and run the pipeline\u001b[39;00m\n\u001b[32m     36\u001b[39m pipeline = dlt.pipeline(\n\u001b[32m     37\u001b[39m     pipeline_name=\u001b[33m\"\u001b[39m\u001b[33mzoomcamp_pipeline\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     38\u001b[39m     destination=\u001b[33m\"\u001b[39m\u001b[33mduckdb\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     39\u001b[39m     dataset_name=\u001b[33m\"\u001b[39m\u001b[33mzoomcamp_tagged_data\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     40\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m load_info = \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzoomcamp_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[38;5;28mprint\u001b[39m(pipeline.last_trace)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/pipeline/pipeline.py:227\u001b[39m, in \u001b[36mwith_runtime_trace.<locals>.decorator.<locals>._wrap\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    224\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trace:\n\u001b[32m    225\u001b[39m         trace_step = start_trace_step(trace, cast(TPipelineStep, f.\u001b[34m__name__\u001b[39m), \u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m     step_info = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    228\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m step_info\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/pipeline/pipeline.py:276\u001b[39m, in \u001b[36mwith_config_section.<locals>.decorator.<locals>._wrap\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    268\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wrap\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mPipeline\u001b[39m\u001b[33m\"\u001b[39m, *args: Any, **kwargs: Any) -> Any:\n\u001b[32m    270\u001b[39m     \u001b[38;5;66;03m# add section context to the container to be used by all configuration without explicit sections resolution\u001b[39;00m\n\u001b[32m    271\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m inject_section(\n\u001b[32m    272\u001b[39m         ConfigSectionContext(\n\u001b[32m    273\u001b[39m             pipeline_name=\u001b[38;5;28mself\u001b[39m.pipeline_name, sections=sections, merge_style=merge_func\n\u001b[32m    274\u001b[39m         )\n\u001b[32m    275\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/pipeline/pipeline.py:707\u001b[39m, in \u001b[36mPipeline.run\u001b[39m\u001b[34m(self, data, destination, staging, dataset_name, credentials, table_name, write_disposition, columns, primary_key, schema, loader_file_format, table_format, schema_contract, refresh)\u001b[39m\n\u001b[32m    700\u001b[39m \u001b[38;5;66;03m# sync state with destination\u001b[39;00m\n\u001b[32m    701\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    702\u001b[39m     \u001b[38;5;28mself\u001b[39m.config.restore_from_destination\n\u001b[32m    703\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dev_mode\n\u001b[32m    704\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state_restored\n\u001b[32m    705\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._destination \u001b[38;5;129;01mor\u001b[39;00m destination)\n\u001b[32m    706\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m707\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sync_destination\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdestination\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstaging\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    708\u001b[39m     \u001b[38;5;66;03m# sync only once\u001b[39;00m\n\u001b[32m    709\u001b[39m     \u001b[38;5;28mself\u001b[39m._state_restored = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/pipeline/pipeline.py:181\u001b[39m, in \u001b[36mwith_schemas_sync.<locals>._wrap\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    179\u001b[39m     \u001b[38;5;28mself\u001b[39m._schema_storage.commit_live_schema(name)\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m     rv = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# because we committed live schema before calling f, we may safely\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# drop all changes in live schemas\u001b[39;00m\n\u001b[32m    185\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m._schema_storage.live_schemas.keys()):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/pipeline/pipeline.py:878\u001b[39m, in \u001b[36mPipeline._sync_destination\u001b[39m\u001b[34m(self, destination, staging, dataset_name)\u001b[39m\n\u001b[32m    876\u001b[39m     \u001b[38;5;28mself\u001b[39m._save_state(state)\n\u001b[32m    877\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PipelineStepFailed(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33msync\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m, ex, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mex\u001b[39;00m\n",
      "\u001b[31mPipelineStepFailed\u001b[39m: Pipeline execution failed at `step=sync` with exception:\n\n<class 'dlt.common.exceptions.MissingDependencyException'>\n\nYou must install additional dependencies to run `duckdb destination`. If you use pip you may do the following:\n\npip install \"dlt[duckdb]\"\n\nDependencies for specific destinations are available as extras of dlt"
     ]
    }
   ],
   "source": [
    "import dlt\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Step 1: Create DLT resource\n",
    "@dlt.resource(write_disposition=\"replace\", name=\"zoomcamp_data\")\n",
    "def zoomcamp_data():\n",
    "    url = \"https://us-central1-dlthub-analytics.cloudfunctions.net/data_engineering_zoomcamp_api\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    df['Trip_Pickup_DateTime'] = pd.to_datetime(df['Trip_Pickup_DateTime'])\n",
    "\n",
    "    # Define buckets\n",
    "    df['tag'] = pd.cut(\n",
    "        df['Trip_Pickup_DateTime'],\n",
    "        bins=[\n",
    "            pd.Timestamp(\"2009-06-01\"),\n",
    "            pd.Timestamp(\"2009-06-10\"),\n",
    "            pd.Timestamp(\"2009-06-20\"),\n",
    "            pd.Timestamp(\"2009-06-30\")\n",
    "        ],\n",
    "        labels=[\"first_10_days\", \"second_10_days\", \"last_10_days\"],\n",
    "        right=False\n",
    "    )\n",
    "\n",
    "    # Drop rows not in the specified range\n",
    "    df = df[df['tag'].notnull()]\n",
    "    yield df\n",
    "\n",
    "\n",
    "# Step 2: Create and run the pipeline\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"zoomcamp_pipeline\",\n",
    "    destination=\"duckdb\",\n",
    "    dataset_name=\"zoomcamp_tagged_data\"\n",
    ")\n",
    "load_info = pipeline.run(zoomcamp_data())\n",
    "print(pipeline.last_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c3268c-a2d5-464a-b6cb-0a3ff4220c8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
