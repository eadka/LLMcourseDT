{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a75230de-8ba9-4748-a3fa-b326ea4207df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: dlt\n",
      "Version: 1.12.3\n",
      "Summary: dlt is an open-source python-first scalable data loading library that does not require any backend to run.\n",
      "Home-page: https://github.com/dlt-hub\n",
      "Author: \n",
      "Author-email: \"dltHub Inc.\" <services@dlthub.com>\n",
      "License-Expression: Apache-2.0\n",
      "Location: /usr/local/python/3.12.1/lib/python3.12/site-packages\n",
      "Requires: click, fsspec, gitpython, giturlparse, hexbytes, humanize, jsonpath-ng, orjson, packaging, pathvalidate, pendulum, pluggy, pytz, pyyaml, requests, requirements-parser, rich-argparse, semver, setuptools, simplejson, sqlglot, tenacity, tomlkit, typing-extensions, tzdata\n",
      "Required-by: cognee\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show dlt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfabbc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q \"dlt[qdrant]\" \"qdrant-client[fastembed]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11b68a2f-aae2-4ed2-b487-240ca13051fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Step 1: Create DLT resource\n",
    "@dlt.resource(write_disposition=\"replace\", name=\"zoomcamp_data\")\n",
    "def zoomcamp_data():\n",
    "    docs_url = 'https://github.com/alexeygrigorev/llm-rag-workshop/raw/main/notebooks/documents.json'\n",
    "    docs_response = requests.get(docs_url)\n",
    "    documents_raw = docs_response.json()\n",
    "\n",
    "    for course in documents_raw:\n",
    "        course_name = course['course']\n",
    "\n",
    "        for doc in course['documents']:\n",
    "            doc['course'] = course_name\n",
    "            yield doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0c3268c-a2d5-464a-b6cb-0a3ff4220c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run started at 2025-07-08 00:27:43.991045+00:00 and COMPLETED in 3.22 seconds with 4 steps.\n",
      "Step extract COMPLETED in 0.94 seconds.\n",
      "\n",
      "Load package 1751934464.3615549 is EXTRACTED and NOT YET LOADED to the destination and contains no failed jobs\n",
      "\n",
      "Step normalize COMPLETED in 0.48 seconds.\n",
      "Normalized data for the following tables:\n",
      "- zoomcamp_data: 948 row(s)\n",
      "- _dlt_pipeline_state: 1 row(s)\n",
      "\n",
      "Load package 1751934464.3615549 is NORMALIZED and NOT YET LOADED to the destination and contains no failed jobs\n",
      "\n",
      "Step load COMPLETED in 1.47 seconds.\n",
      "Pipeline zoomcamp_pipeline load step completed in 0.51 seconds\n",
      "1 load package(s) were loaded to destination duckdb and into dataset zoomcamp_tagged_data\n",
      "The duckdb destination used duckdb:////workspaces/LLMcourseDT/dlt_workshop/zoomcamp_pipeline.duckdb location to store data\n",
      "Load package 1751934464.3615549 is LOADED and contains no failed jobs\n",
      "\n",
      "Step run COMPLETED in 3.22 seconds.\n",
      "Pipeline zoomcamp_pipeline load step completed in 0.51 seconds\n",
      "1 load package(s) were loaded to destination duckdb and into dataset zoomcamp_tagged_data\n",
      "The duckdb destination used duckdb:////workspaces/LLMcourseDT/dlt_workshop/zoomcamp_pipeline.duckdb location to store data\n",
      "Load package 1751934464.3615549 is LOADED and contains no failed jobs\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Create and run the pipeline\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"zoomcamp_pipeline\",\n",
    "    destination=\"duckdb\",\n",
    "    dataset_name=\"zoomcamp_tagged_data\"\n",
    ")\n",
    "load_info = pipeline.run(zoomcamp_data())\n",
    "print(pipeline.last_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49d2d7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2\n",
    "from dlt.destinations import qdrant\n",
    "\n",
    "qdrant_destination = qdrant(\n",
    "  qd_path=\"db.qdrant\", \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "932716cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-08 00:28:54,969|[WARNING]|5183|131541469557376|dlt|pipeline.py|_state_to_props:1680|The destination dlt.destinations.duckdb:None in state differs from destination dlt.destinations.qdrant:qdrant in pipeline and will be ignored\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<00:00,  7.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run started at 2025-07-08 00:28:54.980956+00:00 and COMPLETED in 11.01 seconds with 4 steps.\n",
      "Step extract COMPLETED in 0.26 seconds.\n",
      "\n",
      "Load package 1751934539.4975917 is EXTRACTED and NOT YET LOADED to the destination and contains no failed jobs\n",
      "\n",
      "Step normalize COMPLETED in 0.08 seconds.\n",
      "Normalized data for the following tables:\n",
      "- zoomcamp_data: 948 row(s)\n",
      "- _dlt_pipeline_state: 1 row(s)\n",
      "\n",
      "Load package 1751934539.4975917 is NORMALIZED and NOT YET LOADED to the destination and contains no failed jobs\n",
      "\n",
      "Step load COMPLETED in 6.31 seconds.\n",
      "Pipeline zoomcamp_pipeline load step completed in 6.29 seconds\n",
      "1 load package(s) were loaded to destination qdrant and into dataset zoomcamp_tagged_data\n",
      "The qdrant destination used /workspaces/LLMcourseDT/dlt_workshop/db.qdrant location to store data\n",
      "Load package 1751934539.4975917 is LOADED and contains no failed jobs\n",
      "\n",
      "Step run COMPLETED in 11.01 seconds.\n",
      "Pipeline zoomcamp_pipeline load step completed in 6.29 seconds\n",
      "1 load package(s) were loaded to destination qdrant and into dataset zoomcamp_tagged_data\n",
      "The qdrant destination used /workspaces/LLMcourseDT/dlt_workshop/db.qdrant location to store data\n",
      "Load package 1751934539.4975917 is LOADED and contains no failed jobs\n"
     ]
    }
   ],
   "source": [
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"zoomcamp_pipeline\",\n",
    "    destination=qdrant_destination,\n",
    "    dataset_name=\"zoomcamp_tagged_data\"\n",
    "\n",
    ")\n",
    "load_info = pipeline.run(zoomcamp_data())\n",
    "print(pipeline.last_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bebae3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
